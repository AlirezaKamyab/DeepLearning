{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ef4287",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48caba8",
   "metadata": {},
   "source": [
    "## Notations and demonstrations\n",
    "- $m$ is denoted as the number of training examples\n",
    "- $n$ is denoted as the number of the features\n",
    "- $X$ is a matrix that each column is a training example and each row is a feature <br>\n",
    "$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "    \\displaystyle\n",
    "    x^{(1)}_{1} & x^{(2)}_{1} & \\dots & x^{(m)}_{1} \\\\\n",
    "    x^{(1)}_{2} & x^{(2)}_{2} & \\dots & x^{(m)}_{2} \\\\\n",
    "    & \\vdots & \\vdots & \\\\\n",
    "    x^{(1)}_{n} & x^{(2)}_{n} & \\dots & x^{(m)}_{n} \\\\\n",
    "\\end{bmatrix}_{n_x \\times m}\n",
    "$\n",
    "- $y$ is a vector denoted as the labels <br>\n",
    "$\n",
    "y = \n",
    "\\begin{bmatrix}\n",
    "    y_1 & y_2 & \\dots & y_m\n",
    "\\end{bmatrix}\n",
    "$\n",
    "- $w$ is a vector of weights <br>\n",
    "$\n",
    "w = \n",
    "\\begin{bmatrix}\n",
    "\\displaystyle\n",
    "    w_1\\\\\n",
    "    w_2\\\\\n",
    "    \\vdots\\\\\n",
    "    w_n\n",
    "\\end{bmatrix}\n",
    "$\n",
    "<br> <br>\n",
    "Neural networks might have multiple layers; each layers contains multiple perceptron and each themselves could be logistic function or ...; so for each we need different vector of weights that could be obtained via stacking each function's weights in a matrix;\n",
    "<br>\n",
    "- $W^{[1]}$ is a matrix for the first layer of the neural network; for example, if the first layer contains 4 logistic functions we have<br>\n",
    "$\n",
    "W^{[1]} =\n",
    "\\displaystyle\n",
    "\\begin{bmatrix}\n",
    "    \\dots & w_1^{[1]T} & \\dots\\\\\n",
    "    \\dots & w_2^{[1]T} & \\dots\\\\\n",
    "    \\dots & w_3^{[1]T} & \\dots\\\\\n",
    "    \\dots & w_4^{[1]T} & \\dots\\\\\n",
    "\\end{bmatrix}_{4, n}\n",
    "$\n",
    "<br><br>\n",
    "Since we have multiple functions in each layer, we need multiple intercepts.\n",
    "- $b$ is vector of intercepts; If we have 4 logistic function in the first layer, it would be like below <br>\n",
    "$\n",
    "b^{[1]} = \n",
    "\\displaystyle\n",
    "\\begin{bmatrix}\n",
    "    b_1^{[1]}\\\\\n",
    "    b_2^{[1]}\\\\\n",
    "    b_3^{[1]}\\\\\n",
    "    b_4^{[1]}\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "<br><br>\n",
    "For calculating $Z$ we have\n",
    "$$\n",
    "Z^{[1]} = \n",
    "\\begin{bmatrix}\n",
    "    \\dots & w_1^{[1]T} & \\dots\\\\\n",
    "    \\dots & w_2^{[1]T} & \\dots\\\\\n",
    "    \\dots & w_3^{[1]T} & \\dots\\\\\n",
    "    \\dots & w_4^{[1]T} & \\dots\\\\\n",
    "\\end{bmatrix}_{4, n}\n",
    "\\begin{bmatrix}\n",
    "    \\displaystyle\n",
    "    x^{(1)} & x^{(2)} & \\dots & x^{(m)}\n",
    "\\end{bmatrix}_{n_x \\times m} + \n",
    "\\begin{bmatrix}\n",
    "    b_1^{[1]}\\\\\n",
    "    b_2^{[1]}\\\\\n",
    "    b_3^{[1]}\\\\\n",
    "    b_4^{[1]}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The result will be\n",
    "$$\n",
    "Z^{[1]} = \n",
    "\\begin{bmatrix}\n",
    "    \\displaystyle\n",
    "    W_1^{[1]T}.x^{(1)} + b_1^{[1]}\\\\\n",
    "    W_2^{[1]T}.x^{(2)} + b_2^{[1]}\\\\\n",
    "    W_3^{[1]T}.x^{(3)} + b_3^{[1]}\\\\\n",
    "    W_4^{[1]T}.x^{(4)} + b_4^{[1]}\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    \\displaystyle\n",
    "    Z_1^{[1]}\\\\\n",
    "    Z_2^{[1]}\\\\\n",
    "    Z_3^{[1]}\\\\\n",
    "    Z_4^{[1]}\\\\\n",
    "\\end{bmatrix}_{4, m}\n",
    "$$\n",
    "\n",
    "Applying the activation function for the first layer containing 4 perceptrons we have\n",
    "$$\n",
    "a^{[1]} = \n",
    "\\begin{bmatrix}\n",
    "    \\displaystyle\n",
    "    a_1^{[1]}\\\\\n",
    "    a_2^{[1]}\\\\\n",
    "    a_3^{[1]}\\\\\n",
    "    a_4^{[1]}\\\\\n",
    "\\end{bmatrix}\n",
    "= G(Z^{[1]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d553824",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "- Sigmoid\n",
    "$$\n",
    "    \\begin{equation}\n",
    "        a = g(z) = \\displaystyle\\frac{1}{1 + e^{-z}} \\\\\n",
    "    \\end{equation}\n",
    "$$\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" width=\"300px\" height=\"300px\"/>\n",
    "\n",
    "    - Derivative\n",
    "    $$\n",
    "        \\begin{equation}\n",
    "            g'(z) = \\frac{dg(z)}{dz} = \\frac{1}{1 + e^{-z}}(1 - \\frac{1}{1 + e^{-z}}) \\\\ \n",
    "            g'(z) = g(z)(1 - g(z)) = a(1 - a)\n",
    "        \\end{equation}\n",
    "    $$\n",
    "    Calculations :\n",
    "    $$\n",
    "        \\begin{equation}\n",
    "            \\frac{d}{dz}g(z) = \\frac{0 - (-e^{-z})}{(1 + e^{-z})^2} = \\frac{e^{-z}}{(1 + e^{-z})^2} = \\\\\n",
    "                \\frac{e^{-z} + 1 - 1}{1 + e^{-z}} \\times \\frac{1}{1 + e^{-z}} = \\\\\n",
    "                (1 - \\frac{1}{1 + e^{-z}})\\frac{1}{1+e^{-z}}\n",
    "        \\end{equation}\n",
    "    $$\n",
    "    <br><br>\n",
    "\n",
    "- tanh\n",
    "$$\n",
    "    \\begin{equation}\n",
    "        a = g(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "    \\end{equation}\n",
    "$$\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/76/Sinh_cosh_tanh.svg\" width=\"300px\" height=\"300px\"/>\n",
    "\n",
    "    - Derivative\n",
    "    $$\n",
    "        \\begin{equation}\n",
    "        g'(z) = \\frac{d}{dz}tanh(z) = 1 - (tanh(z))^2\\\\\n",
    "        g'(z) = 1 - a^2\n",
    "        \\end{equation}\n",
    "    $$\n",
    "    Calculations:\n",
    "    $$\n",
    "        \\begin{equation}\n",
    "            \\frac{d}{dz} tanh(z) = \\frac{(e^z+e^{-z}) - (e^z-e^{-z})}{(e^z+e^{-z})^2} = \\\\\n",
    "            1 - \\frac{(e^z-e^{-z})}{e^z+e^{-z}} = 1 - (tanh(z))^2\n",
    "        \\end{equation}\n",
    "    $$\n",
    "\n",
    "<br><br>\n",
    "- ReLU\n",
    "$$\n",
    "    a = max(0, z)\n",
    "$$\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/42/ReLU_and_GELU.svg\" width=\"300px\" height=\"300px\"/>\n",
    "\n",
    "    - Derivative\n",
    "    $$\n",
    "    g'(z) = \n",
    "        \\begin{equation}\n",
    "            \\begin{cases}\n",
    "            0 & z < 0\\\\\n",
    "            1 & z \\ge 0\n",
    "            \\end{cases}\n",
    "        \\end{equation}\n",
    "    $$\n",
    "    \n",
    "<br><br>\n",
    "- leaky ReLU\n",
    "$$\n",
    "    a = max(0.01z, z)\n",
    "$$\n",
    "\n",
    "    - Derivative\n",
    "    \n",
    "    $$\n",
    "    g'(z) = \n",
    "        \\begin{equation}\n",
    "            \\begin{cases}\n",
    "            0.01 & z < 0\\\\\n",
    "            1 & z \\ge 0\n",
    "            \\end{cases}\n",
    "        \\end{equation}\n",
    "    $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d445d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141d79f",
   "metadata": {},
   "source": [
    "## Implement L-Layered Neural Network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0736d04",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba50c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\" \n",
    "    arguments:\n",
    "        n_x is the size of input layer\n",
    "        n_h is the size of hidden layers\n",
    "        n_y is the size of output layer\n",
    "    returns:\n",
    "        W1 is a matrix of weights for the first layer of the NN, shape=(n_h, n_x)\n",
    "        b1 is a vector of intercepts for the first layer of NN, shape=(n_h, 1)\n",
    "        W2 is a matrix of weights for the second layer of the NN, shape=(n_y, n_h)\n",
    "        b2 is a vector of intercepts for the second layer of the NN, shape=(n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    params = {\"W1\":W1,\n",
    "              \"b1\":b1,\n",
    "              \"W2\":W2,\n",
    "              \"b2\":b2}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0176c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 :\n",
      "[[-2.05635676e-02 -3.56257408e-03 -7.16440517e-03 -1.51355083e-02]\n",
      " [ 3.65237917e-03  1.12983080e-02  1.18881992e-02  1.11196012e-02]\n",
      " [ 1.65494760e-03 -1.51354204e-05  1.19140318e-03 -6.23067487e-03]\n",
      " [-1.28221001e-02 -3.90151885e-03  4.03808821e-03  2.24828296e-03]\n",
      " [-2.32608847e-03  1.69275231e-03  1.98555883e-03 -4.08751622e-03]]\n",
      "b1 :\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 :\n",
      "[[-0.01849866 -0.00469481 -0.01029114  0.01751907  0.01047541]\n",
      " [-0.01411954  0.00605058 -0.00198229 -0.00612071 -0.00748033]\n",
      " [ 0.00055922  0.00470869 -0.01111253 -0.00044706 -0.00909943]]\n",
      "b2 :\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test function\n",
    "params = initialize_parameters(4, 5, 3)\n",
    "for k, v in params.items():\n",
    "    print(k, \":\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e042caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        layer_dims: A list that contains the size of each layer\n",
    "        \n",
    "    returns:\n",
    "        dict: parameters for each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {}\n",
    "    L = len(layer_dims) # number of layers\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        params[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        params[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639e330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 :\n",
      "[[-0.0115657   0.00360858  0.00295114]\n",
      " [ 0.02942256 -0.00571661  0.00509639]\n",
      " [-0.01574587 -0.00864654  0.00670793]\n",
      " [ 0.00788212 -0.00356852 -0.00956044]\n",
      " [ 0.01647727 -0.00797142 -0.00980176]]\n",
      "b1 :\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 :\n",
      "[[ 7.31405629e-03 -2.98410968e-03  9.23645827e-05  2.11270800e-03\n",
      "   1.43781742e-02]\n",
      " [-1.16380391e-02 -1.05307857e-02 -9.46954877e-03  5.07818544e-03\n",
      "  -5.97416338e-03]\n",
      " [-6.67101356e-03 -1.82783248e-03 -9.49261239e-03 -2.01604609e-02\n",
      "  -3.34573958e-03]\n",
      " [ 3.86915976e-03 -4.96848409e-03 -2.98154004e-03  1.24279286e-02\n",
      "   5.13886513e-03]]\n",
      "b2 :\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 :\n",
      "[[-0.00921872  0.01960317 -0.00783147 -0.00435459]]\n",
      "b3 :\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test function\n",
    "params = initialize_parameters_deep([3, 5, 4, 1])\n",
    "for k, v in params.items():\n",
    "    print(k, \":\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428b907",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31941956",
   "metadata": {},
   "source": [
    "#### Linear forward\n",
    "Here we need to calculate $Z^{[l]} = W^{[l]}.A^{[l-1]} + b^{[l]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4871a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        A: activations from previous layer or inputs, shape=(size of l-1, m)\n",
    "        W: weights from layer l, shape=(size of layer l, size of layer l-1)\n",
    "        b: intercepts from layer l, shape(size of layer l, 1)\n",
    "        \n",
    "    returns:\n",
    "        Z: forward calculation\n",
    "        cache: The same arguments useful for backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.matmul(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7834cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01565665  0.00912571  0.00129837  0.02729514]\n",
      " [-0.0448115  -0.01547954  0.01633857 -0.06643729]\n",
      " [ 0.0219114   0.00930118 -0.00285427  0.02995895]\n",
      " [-0.00779827 -0.0105304  -0.0120543  -0.01986947]\n",
      " [-0.01994317 -0.016868   -0.01136909 -0.04092758]]\n",
      "[[-1.48263878 -0.55166167  0.43448755 -2.16660949]\n",
      " [-0.11604555  0.33387478  0.61643479  0.54842977]\n",
      " [-0.36336679  0.52201538  1.38897524  0.08733081]]\n",
      "[[-0.0115657   0.00360858  0.00295114]\n",
      " [ 0.02942256 -0.00571661  0.00509639]\n",
      " [-0.01574587 -0.00864654  0.00670793]\n",
      " [ 0.00788212 -0.00356852 -0.00956044]\n",
      " [ 0.01647727 -0.00797142 -0.00980176]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(3, 4)\n",
    "Y = np.array([[1, 0, 1, 0]])\n",
    "Z, cache = linear_forward(X, params['W1'], params['b1'])\n",
    "print(Z)\n",
    "print(cache[0])\n",
    "print(cache[1])\n",
    "print(cache[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e9c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        Z: a matrix calculated in \"linear_forward\" step\n",
    "        \n",
    "    returns:\n",
    "        A: activation function of sigmoid(Z)\n",
    "        cache: The same arguments useful for backward propagation\n",
    "    \"\"\"\n",
    "     \n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7763be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        Z: a matrix calculated in \"linear_forward\" step\n",
    "        \n",
    "    returns:\n",
    "        A: activation function of relu(Z)\n",
    "        cache: The same arguments useful for backward propagation\n",
    "    \"\"\"\n",
    "     \n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49a3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        A: activations from previous layer or inputs, shape=(size of l-1, m)\n",
    "        W: weights from layer l, shape=(size of layer l, size of layer l-1)\n",
    "        b: intercepts from layer l, shape(size of layer l, 1)\n",
    "        activation: A string that specifies what type of activiation function to use (sigmoid or ReLU)\n",
    "    \n",
    "    returns:\n",
    "        A: activation calculated\n",
    "        cache: to be used for backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed1272e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01565665 0.00912571 0.00129837 0.02729514]\n",
      " [0.         0.         0.01633857 0.        ]\n",
      " [0.0219114  0.00930118 0.         0.02995895]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "(array([[-1.48263878, -0.55166167,  0.43448755, -2.16660949],\n",
      "       [-0.11604555,  0.33387478,  0.61643479,  0.54842977],\n",
      "       [-0.36336679,  0.52201538,  1.38897524,  0.08733081]]), array([[-0.0115657 ,  0.00360858,  0.00295114],\n",
      "       [ 0.02942256, -0.00571661,  0.00509639],\n",
      "       [-0.01574587, -0.00864654,  0.00670793],\n",
      "       [ 0.00788212, -0.00356852, -0.00956044],\n",
      "       [ 0.01647727, -0.00797142, -0.00980176]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]))\n",
      "[[ 0.01565665  0.00912571  0.00129837  0.02729514]\n",
      " [-0.0448115  -0.01547954  0.01633857 -0.06643729]\n",
      " [ 0.0219114   0.00930118 -0.00285427  0.02995895]\n",
      " [-0.00779827 -0.0105304  -0.0120543  -0.01986947]\n",
      " [-0.01994317 -0.016868   -0.01136909 -0.04092758]]\n"
     ]
    }
   ],
   "source": [
    "A, cache = linear_activation_forward(X, params[\"W1\"], params[\"b1\"], activation='relu')\n",
    "print(A)\n",
    "print(cache[0])\n",
    "print(cache[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa7a161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        X: numpy array, shape=(input size, input examples i.e m)\n",
    "        parameters: output of initialize_parameters_deep\n",
    "        \n",
    "    returns:\n",
    "        AL: activation value from the output layer\n",
    "        caches: Caches from calling Linear activation forward, the size is L since there are L layers\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    caches = []\n",
    "    A_prev = X\n",
    "    \n",
    "    # Do forward propagation with relu activation function for layers from 1 to l-1\n",
    "    for l in range(1, L):\n",
    "        Wl = parameters['W' + str(l)]\n",
    "        bl = parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, Wl, bl, 'relu')\n",
    "        caches.append(cache)\n",
    "        A_prev = A\n",
    "        \n",
    "    # Do forward propagation with sigmoid activation function for layer L\n",
    "    Wl = parameters['W' + str(L)]\n",
    "    bl = parameters['b' + str(L)]\n",
    "    A, cache = linear_activation_forward(A_prev, Wl, bl, 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9f8aa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49999973 0.49999984 0.5        0.49999952]]\n",
      "\n",
      "\n",
      "[((array([[-1.48263878, -0.55166167,  0.43448755, -2.16660949],\n",
      "       [-0.11604555,  0.33387478,  0.61643479,  0.54842977],\n",
      "       [-0.36336679,  0.52201538,  1.38897524,  0.08733081]]), array([[-0.0115657 ,  0.00360858,  0.00295114],\n",
      "       [ 0.02942256, -0.00571661,  0.00509639],\n",
      "       [-0.01574587, -0.00864654,  0.00670793],\n",
      "       [ 0.00788212, -0.00356852, -0.00956044],\n",
      "       [ 0.01647727, -0.00797142, -0.00980176]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[ 0.01565665,  0.00912571,  0.00129837,  0.02729514],\n",
      "       [-0.0448115 , -0.01547954,  0.01633857, -0.06643729],\n",
      "       [ 0.0219114 ,  0.00930118, -0.00285427,  0.02995895],\n",
      "       [-0.00779827, -0.0105304 , -0.0120543 , -0.01986947],\n",
      "       [-0.01994317, -0.016868  , -0.01136909, -0.04092758]])), ((array([[0.01565665, 0.00912571, 0.00129837, 0.02729514],\n",
      "       [0.        , 0.        , 0.01633857, 0.        ],\n",
      "       [0.0219114 , 0.00930118, 0.        , 0.02995895],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        ]]), array([[ 7.31405629e-03, -2.98410968e-03,  9.23645827e-05,\n",
      "         2.11270800e-03,  1.43781742e-02],\n",
      "       [-1.16380391e-02, -1.05307857e-02, -9.46954877e-03,\n",
      "         5.07818544e-03, -5.97416338e-03],\n",
      "       [-6.67101356e-03, -1.82783248e-03, -9.49261239e-03,\n",
      "        -2.01604609e-02, -3.34573958e-03],\n",
      "       [ 3.86915976e-03, -4.96848409e-03, -2.98154004e-03,\n",
      "         1.24279286e-02,  5.13886513e-03]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[ 1.16537474e-04,  6.76050776e-05, -3.92597530e-05,\n",
      "         2.02405359e-04],\n",
      "       [-3.89703770e-04, -1.94283335e-04, -1.87168480e-04,\n",
      "        -6.01359667e-04],\n",
      "       [-3.12442134e-04, -1.49170204e-04, -3.85256102e-05,\n",
      "        -4.66474957e-04],\n",
      "       [-4.75161674e-06,  7.57701566e-06, -7.61543456e-05,\n",
      "         1.62854639e-05]])), ((array([[1.16537474e-04, 6.76050776e-05, 0.00000000e+00, 2.02405359e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 7.57701566e-06, 0.00000000e+00, 1.62854639e-05]]), array([[-0.00921872,  0.01960317, -0.00783147, -0.00435459]]), array([[0.]])), array([[-1.07432673e-06, -6.56227274e-07,  0.00000000e+00,\n",
      "        -1.93683546e-06]]))]\n"
     ]
    }
   ],
   "source": [
    "AL, caches = L_model_forward(X, params)\n",
    "print(AL)\n",
    "print('\\n')\n",
    "print(caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c05f2",
   "metadata": {},
   "source": [
    "#### Compute Cost\n",
    "Cost for sigmoid function is defined as\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53656f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        AL: nparray; the output of the neural network\n",
    "        Y: array of labels\n",
    "        \n",
    "    returns:\n",
    "        decimal number specifing the cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[-1]\n",
    "    cost = (np.matmul(Y, np.log(AL).T) + np.matmul(1 - Y, np.log(1 - AL).T)) / -m\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78b6a932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.69314699)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function\n",
    "compute_cost(AL, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9670b8",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7aec0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        dA: post-activation gradient, of any shape\n",
    "        cache: it is \"Z\" that we stored to use later in back prop\n",
    "    \n",
    "    returns:\n",
    "        derivative of loss function with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "854f7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        dA: post-activation gradient, of any shape\n",
    "        cache: it is \"Z\" that we stored to use later in back prop\n",
    "    \n",
    "    returns:\n",
    "        derivative of loss function with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a77b6",
   "metadata": {},
   "source": [
    "Let's say we have dZ, now we are to find:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a5b1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        dZ: gradient of the cost with respect to the linear output i.e Z\n",
    "        cache: the tupe (A_prev, W, b)\n",
    "    \n",
    "    returns:\n",
    "        dW: gradient of the cost with respect to the weights i.e W\n",
    "        db: gradient of the cost with respect to the intercepts i.e b\n",
    "        dA_prev: gradient of the cost with respect to the activation function in the previous layer\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * np.matmul(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.matmul(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cdf1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        dA: post-activation gradient for current layer l\n",
    "        cache: tuple contaning (linear_cache, activation_cache)\n",
    "        activation: activation function go to be used in this layer\n",
    "        \n",
    "    returns:\n",
    "        dW: gradient of the cost with respect to the weights i.e W\n",
    "        db: gradient of the cost with respect to the intercepts i.e b\n",
    "        dA_prev: gradient of the cost with respect to the activation function in the previous layer\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55c96dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "        AL: predicted outputs from the neural network\n",
    "        Y: correct values\n",
    "        caches: cached values in each step of linear forward and linear activation forward\n",
    "        \n",
    "    returns:\n",
    "        gradients that are --> dA_prev, dW, db for each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    m = AL.shape[1]\n",
    "    L = len(caches)\n",
    "    \n",
    "    dAL = -np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)\n",
    "    current_cache = caches[L - 1]\n",
    "    dA_prev, dW, db = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "    grads['dA' + str(L - 1)] = dA_prev\n",
    "    grads['dW' + str(L)] = dW\n",
    "    grads['db' + str(L)] = db\n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward(grads['dA' + str(l + 1)], current_cache, 'relu')\n",
    "        grads['dA' + str(l)] = dA_prev\n",
    "        grads['dW' + str(l + 1)] = dW\n",
    "        grads['db' + str(l + 1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80dde1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA2': array([[ 0.00460936,  0.00460936,  0.00460936,  0.00460936],\n",
       "        [-0.00980159, -0.00980158, -0.00980158, -0.00980157],\n",
       "        [ 0.00391574,  0.00391573,  0.00391573,  0.00391573],\n",
       "        [ 0.00217729,  0.00217729,  0.00217729,  0.00217729]]),\n",
       " 'dW3': array([[-4.83184693e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "         -2.98280767e-06]]),\n",
       " 'db3': array([[-0.49999991]]),\n",
       " 'dA1': array([[ 3.37131487e-05,  4.21374118e-05,  0.00000000e+00,\n",
       "          4.21373848e-05],\n",
       "        [-1.37548481e-05, -2.45726791e-05,  0.00000000e+00,\n",
       "         -2.45726633e-05],\n",
       "        [ 4.25741994e-07, -6.06594296e-06,  0.00000000e+00,\n",
       "         -6.06593907e-06],\n",
       "        [ 9.73824048e-06,  3.67974671e-05,  0.00000000e+00,\n",
       "          3.67974435e-05],\n",
       "        [ 6.62742404e-05,  7.74629952e-05,  0.00000000e+00,\n",
       "          7.74629456e-05]]),\n",
       " 'dW2': array([[6.00109934e-05, 0.00000000e+00, 7.04903916e-05, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [1.98247041e-05, 0.00000000e+00, 2.13701822e-05, 0.00000000e+00,\n",
       "         0.00000000e+00]]),\n",
       " 'db2': array([[0.00345702],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.00108865]]),\n",
       " 'dA0': array([[-3.96619956e-07, -3.91835236e-07,  0.00000000e+00,\n",
       "         -3.91834985e-07],\n",
       "        [ 1.17975520e-07,  2.04505769e-07,  0.00000000e+00,\n",
       "          2.04505638e-07],\n",
       "        [ 1.02348214e-07,  8.36636831e-08,  0.00000000e+00,\n",
       "          8.36636295e-08]]),\n",
       " 'dW1': array([[-4.11313187e-05,  8.31643856e-06,  3.35650765e-06],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 3.96441195e-06, -1.35035310e-06, -9.62739853e-07],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]),\n",
       " 'db1': array([[ 2.94969863e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [-2.92653501e-06],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function\n",
    "grads = L_model_backward(AL, Y, caches)\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f7a8a",
   "metadata": {},
   "source": [
    "### Update Parameters\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93327cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    arguments\n",
    "        params: is the parameters that we initialized\n",
    "        grads: gradients that are calculated in \"L_model_backward\"\n",
    "        learning_rate: a decimal illustrating how much of the gradient should effect params\n",
    "    \n",
    "    return:\n",
    "        updated parameters\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] = params['W' + str(l)] - learning_rate * grads['dW' + str(l)]\n",
    "        parameters['b' + str(l)] = params['b' + str(l)] - learning_rate * grads['db' + str(l)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a54e8247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.01156529,  0.0036085 ,  0.00295111],\n",
       "        [ 0.02942256, -0.00571661,  0.00509639],\n",
       "        [-0.01574591, -0.00864652,  0.00670794],\n",
       "        [ 0.00788212, -0.00356852, -0.00956044],\n",
       "        [ 0.01647727, -0.00797142, -0.00980176]]),\n",
       " 'b1': array([[-2.94969863e-07],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 2.92653501e-08],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00]]),\n",
       " 'W2': array([[ 7.31345618e-03, -2.98410968e-03,  9.16596787e-05,\n",
       "          2.11270800e-03,  1.43781742e-02],\n",
       "        [-1.16380391e-02, -1.05307857e-02, -9.46954877e-03,\n",
       "          5.07818544e-03, -5.97416338e-03],\n",
       "        [-6.67101356e-03, -1.82783248e-03, -9.49261239e-03,\n",
       "         -2.01604609e-02, -3.34573958e-03],\n",
       "        [ 3.86896151e-03, -4.96848409e-03, -2.98175375e-03,\n",
       "          1.24279286e-02,  5.13886513e-03]]),\n",
       " 'b2': array([[-3.45702037e-05],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00],\n",
       "        [-1.08864587e-05]]),\n",
       " 'W3': array([[-0.00921872,  0.01960317, -0.00783147, -0.00435459]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function\n",
    "update_parameters(params, grads, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
